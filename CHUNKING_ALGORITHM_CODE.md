# RAGFlow ä¸‰å¤§åˆ†å—ç®—æ³•ä»£ç å®ç°è¯¦è§£

## ğŸ“ æºä»£ç ä½ç½®
```
RAGFlow é¡¹ç›®
â”œâ”€ /rag/nlp/__init__.py          â† ä¸‰ä¸ªç®—æ³•éƒ½åœ¨è¿™é‡Œ
â”œâ”€ /rag/nlp/rag_tokenizer.py     â† Token è®¡æ•°å‡½æ•°
â””â”€ /rag/utils/doc_store_conn.py  â† æ•°æ®åº“è¿æ¥
```

---

## 1ï¸âƒ£ naive_merge - ç®€å•åˆ†å—ç®—æ³•

### ç®—æ³•æ€æƒ³
```
é€å¥è¯»ï¼Œç´¯ç§¯tokenï¼Œåˆ°è¾¾é™åˆ¶å°±æ–°å»ºå—ï¼Œæ”¯æŒé‡å 
```

### å®Œæ•´ä»£ç å®ç°

```python
def naive_merge(sections: str | list,
                chunk_token_num=128,
                delimiter="\nã€‚ï¼›ï¼ï¼Ÿ",
                overlapped_percent=0):
    """
    Token-based chunk merging algorithm

    æµç¨‹ï¼š
    1. è§£æåˆ†éš”ç¬¦ï¼Œè½¬æˆæ­£åˆ™è¡¨è¾¾å¼
    2. æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬
    3. éå†æ¯ä¸ªéƒ¨åˆ†ï¼Œç´¯ç§¯tokenè®¡æ•°
    4. å½“è¶…è¿‡é™åˆ¶æ—¶ï¼Œåº”ç”¨é‡å å¹¶æ–°å»ºå—

    å‚æ•°è¯´æ˜ï¼š
    - sections: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
    - chunk_token_num: ç›®æ ‡å—å¤§å°ï¼ˆtokenæ•°ï¼‰
    - delimiter: åˆ†éš”ç¬¦ï¼ˆæ”¯æŒå¤šä¸ªï¼Œå¦‚ "\nã€‚ï¼›ï¼ï¼Ÿ"ï¼‰
    - overlapped_percent: å—é‡å æ¯”ä¾‹ï¼ˆ0-100%ï¼‰

    è¿”å›ï¼šchunks åˆ—è¡¨ï¼ˆæ¯ä¸ªå—éƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
    """

    # åˆå§‹åŒ–
    cks = [""]              # å½“å‰å—åˆ—è¡¨
    tk_nums = [0]           # æ¯ä¸ªå—çš„ token æ•°

    # å°†åˆ†éš”ç¬¦è½¬æ¢æˆæ­£åˆ™è¡¨è¾¾å¼
    # ä¾‹å¦‚ "\nã€‚ï¼›ï¼ï¼Ÿ" â†’ regex: "[\nã€‚ï¼›ï¼ï¼Ÿ]"
    delimiter_pattern = f"[{re.escape(delimiter)}]"

    # åˆ†å‰²è¾“å…¥æ–‡æœ¬
    if isinstance(sections, str):
        sections = [sections]

    all_sections = []
    for section in sections:
        parts = re.split(delimiter_pattern, section)
        all_sections.extend(parts)

    # å¤„ç†æ¯ä¸ªéƒ¨åˆ†
    for part in all_sections:
        if not part.strip():  # è·³è¿‡ç©ºéƒ¨åˆ†
            continue

        # è®¡ç®—è¿™ä¸ªéƒ¨åˆ†çš„ token æ•°
        tnum = num_tokens_from_string(part)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å»ºå—
        # æ¡ä»¶1ï¼šå½“å‰å—ä¸ºç©º
        # æ¡ä»¶2ï¼šå½“å‰å—çš„ token æ•°è¶…è¿‡é™åˆ¶
        threshold = chunk_token_num * (100 - overlapped_percent) / 100.0

        if cks[-1] == "" or tk_nums[-1] > threshold:
            # éœ€è¦æ–°å»ºå—

            # å¦‚æœå¯ç”¨äº†é‡å ï¼Œä»å‰ä¸€ä¸ªå—å–æœ«å°¾éƒ¨åˆ†
            if overlapped_percent > 0 and len(cks) > 1:
                prev_chunk = cks[-1]
                # å»æ‰æ ‡ç­¾ï¼ˆPDF å¯èƒ½æœ‰ HTML æ ‡ç­¾ï¼‰
                prev_chunk_clean = remove_html_tags(prev_chunk)
                # å–æœ«å°¾ overlapped_percent% çš„å†…å®¹
                overlap_start_idx = int(len(prev_chunk_clean) *
                                       (100 - overlapped_percent) / 100.0)
                overlap_content = prev_chunk_clean[overlap_start_idx:]

                # æ–°å— = é‡å å†…å®¹ + æ–°éƒ¨åˆ†
                cks.append(overlap_content + part)
            else:
                # æ²¡æœ‰é‡å ï¼Œç›´æ¥æ–°å»ºå—
                cks.append(part)

            tk_nums.append(tnum)
        else:
            # è¿½åŠ åˆ°å½“å‰å—
            cks[-1] += part
            tk_nums[-1] += tnum

    # è¿”å›éç©ºå—
    return [c for c in cks if c.strip()]


# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— token æ•°
def num_tokens_from_string(text: str) -> int:
    """
    ä½¿ç”¨ tiktokenï¼ˆOpenAI çš„ tokenizerï¼‰è®¡ç®— token æ•°

    åŸç†ï¼š
    - è‹±æ–‡ï¼šä¸€èˆ¬ 1 ä¸ªè¯ â‰ˆ 1.3 ä¸ª token
    - ä¸­æ–‡ï¼šä¸€èˆ¬ 1 ä¸ªæ±‰å­— â‰ˆ 1 ä¸ª token

    å®ç°ï¼š
    """
    try:
        # åŠ è½½ OpenAI çš„ tokenizer
        encoding = tiktoken.get_encoding("cl100k_base")
        # ç¼–ç æ–‡æœ¬
        token_integers = encoding.encode(text)
        # è¿”å› token æ•°
        return len(token_integers)
    except:
        # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœ tiktoken ä¸å¯ç”¨ï¼Œç”¨ç®€å•ä¼°ç®—
        # ä¸­æ–‡ï¼šæ±‰å­—æ•° â‰ˆ token æ•°
        # è‹±æ–‡ï¼šå­—æ•° / 4 â‰ˆ token æ•°
        cn_count = len(re.findall(r'[\u4e00-\u9fff]', text))
        en_count = len(text) - cn_count
        return cn_count + en_count // 4


# è¾…åŠ©å‡½æ•°ï¼šå»æ‰ HTML æ ‡ç­¾
def remove_html_tags(text: str) -> str:
    """ç§»é™¤ PDF è§£æå¯èƒ½ç•™ä¸‹çš„ HTML æ ‡ç­¾"""
    pattern = r'<[^>]+>'
    return re.sub(pattern, '', text)
```

### ä»£ç æµç¨‹å›¾

```
è¾“å…¥æ–‡æœ¬
  â†“
æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
  â†“
éå†æ¯ä¸ªæ®µè½
  â”œâ”€ è®¡ç®— token æ•°
  â”œâ”€ æ£€æŸ¥æ˜¯å¦è¶…é™ï¼Ÿ
  â”‚  â”œâ”€ è¶…é™
  â”‚  â”‚  â”œâ”€ å¯ç”¨é‡å ï¼Ÿ
  â”‚  â”‚  â”‚  â”œâ”€ æ˜¯ â†’ å–å‰å—æœ«å°¾ï¼Œè¿æ¥æ–°æ®µè½
  â”‚  â”‚  â”‚  â””â”€ å¦ â†’ ç›´æ¥æ–°å»ºå—
  â”‚  â”‚  â””â”€ ç´¯ç§¯ token æ•°
  â”‚  â””â”€ ä¸è¶…é™
  â”‚     â””â”€ è¿½åŠ åˆ°å½“å‰å—ï¼Œç´¯ç§¯ token æ•°
  â†“
è¾“å‡º chunks åˆ—è¡¨
```

### å®é™…ä¾‹å­ï¼ˆä»£ç æ‰§è¡Œï¼‰

```python
# ç¤ºä¾‹
text = """è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚
å®ƒå¤„ç†æ–‡æœ¬æ•°æ®ã€‚
æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†å‘å±•ã€‚"""

chunks = naive_merge(
    sections=text,
    chunk_token_num=20,        # é™åˆ¶ä¸º 20 token
    delimiter="\nã€‚",          # æŒ‰æ¢è¡Œå’Œå¥å·åˆ†å‰²
    overlapped_percent=20      # 20% é‡å 
)

# æ‰§è¡Œè¿‡ç¨‹ï¼š
# 1. åˆ†å‰²ï¼š["è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯", "å®ƒå¤„ç†æ–‡æœ¬æ•°æ®", "æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†å‘å±•"]
# 2. ç´¯ç§¯ç¬¬1æ®µï¼štoken=12 < 20ï¼Œç»§ç»­
# 3. ç´¯ç§¯ç¬¬2æ®µï¼štoken=12+9=21 > 20ï¼Œè¶…é™ï¼æ–°å»ºå—ï¼Œåº”ç”¨é‡å 
#    â†’ chunk1 = "é‡è¦åˆ†æ”¯ã€‚" + "å®ƒå¤„ç†æ–‡æœ¬æ•°æ®"
# 4. ç´¯ç§¯ç¬¬3æ®µï¼štoken=ç»§ç»­ç´¯ç§¯
# 5. è¾“å‡ºï¼š[chunk1, chunk2, chunk3]
```

---

## 2ï¸âƒ£ hierarchical_merge - å±‚çº§æ„ŸçŸ¥åˆ†å—

### ç®—æ³•æ€æƒ³
```
è¯†åˆ«ç¼–å·è§„åˆ™ â†’ åˆ†é…å±‚çº§ â†’ äºŒåˆ†æŸ¥æ‰¾ â†’ æŒ‰å±‚çº§æ„å»ºå—
```

### å®Œæ•´ä»£ç å®ç°

```python
def hierarchical_merge(bull: int,
                      sections: list,
                      depth: int = 2):
    """
    åŸºäºæ–‡æ¡£ç»“æ„çš„æ™ºèƒ½åˆ†å—

    å‚æ•°è¯´æ˜ï¼š
    - bull: ç¼–å·ç±»å‹
        0 = ä¸­æ–‡ç¼–å·ï¼ˆç¬¬Nç« ã€ç¬¬Næ¡ï¼‰
        1 = é˜¿æ‹‰ä¼¯ç¼–å·ï¼ˆ1. 1.1 1.1.1ï¼‰
        2 = ä¸­æ–‡æ•°å­—ï¼ˆç¬¬ä¸€ã€ç¬¬äºŒã€ç¬¬ä¸‰ï¼‰
        3 = è‹±æ–‡ï¼ˆCHAPTERã€SECTIONï¼‰
        4 = Markdownï¼ˆ#ã€##ã€###ï¼‰
    - sections: [(æ–‡æœ¬, å¸ƒå±€ä¿¡æ¯), ...]
    - depth: æå–åˆ°å“ªä¸€å±‚çº§ï¼ˆ1=ç« ï¼Œ2=èŠ‚ï¼Œ3=å°èŠ‚ï¼‰

    è¿”å›ï¼šchunks åˆ—è¡¨
    """

    # å®šä¹‰ç¼–å·è§„åˆ™
    # BULLET_PATTERN[0] = ä¸­æ–‡ç¼–å·çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
    BULLET_PATTERN = {
        0: [  # ä¸­æ–‡ç¼–å·
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« ]',           # ç¬¬Nç« 
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\.[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[èŠ‚æ¡æ¬¾]',  # ç¬¬N.Mæ¡
            r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)',              # (N)
        ],
        1: [  # é˜¿æ‹‰ä¼¯ç¼–å·
            r'^\d+\.',                    # 1.
            r'^\d+\.\d+',                 # 1.1
            r'^\d+\.\d+\.\d+',            # 1.1.1
        ],
        # ... å…¶ä»–ç¼–å·ç±»å‹ ...
    }

    # Step 1: ä¸ºæ¯ä¸ª section åˆ†é…å±‚çº§
    bullets_size = len(BULLET_PATTERN[bull])
    levels = [[] for _ in range(bullets_size + 2)]
    # levels[i] å­˜å‚¨çš„æ˜¯ç¬¬ i å±‚çº§çš„ section ç´¢å¼•

    for i, (text, layout) in enumerate(sections):
        # å°è¯•åŒ¹é…æ¯ä¸ªç¼–å·æ¨¡å¼
        found = False
        for level_idx, pattern in enumerate(BULLET_PATTERN[bull]):
            if re.match(pattern, text.strip()):
                # æ‰¾åˆ°åŒ¹é…çš„ç¼–å·ï¼Œè®°å½•åœ¨å¯¹åº”å±‚çº§
                levels[level_idx].append(i)
                found = True
                break

        # å¦‚æœæ²¡æœ‰åŒ¹é…ç¼–å·ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜æˆ–å†…å®¹
        if not found:
            if re.search(r'(title|head)', layout):
                # æ˜¯æ ‡é¢˜
                levels[bullets_size].append(i)
            else:
                # æ˜¯æ™®é€šå†…å®¹
                levels[bullets_size + 1].append(i)

    # Step 2: æŒ‰å±‚çº§æ„å»º chunks
    cks = []
    readed = set()  # è®°å½•å·²å¤„ç†è¿‡çš„ section ç´¢å¼•

    for level_idx in range(depth):
        # ä»æœ€é«˜å±‚çº§å¼€å§‹éå†
        arr = levels[level_idx]

        for section_idx in arr:
            if section_idx in readed:
                continue

            # æ–°å»ºä¸€ä¸ª chunkï¼Œä»è¿™ä¸ª section å¼€å§‹
            chunk_items = [section_idx]

            # Step 3: äºŒåˆ†æŸ¥æ‰¾ï¼Œæ‰¾åˆ°è¿™ä¸ª section çš„æ‰€æœ‰å­å†…å®¹
            for lower_level in range(level_idx + 1, len(levels)):
                # åœ¨æ›´ä½å±‚çº§çš„ sections ä¸­äºŒåˆ†æŸ¥æ‰¾
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¤§äºç­‰äº section_idx çš„ç´¢å¼•
                pos = binary_search(levels[lower_level], section_idx)

                if pos >= 0:
                    # æ‰¾åˆ°äº†ï¼Œè¿™æ˜¯å­å†…å®¹
                    child_idx = levels[lower_level][pos]
                    chunk_items.append(child_idx)

            # å°† chunk_items ä¸­çš„å†…å®¹åˆå¹¶
            chunk_text = merge_sections(
                sections,
                chunk_items,
                chunk_token_num=512
            )

            cks.append(chunk_text)

            # æ ‡è®°å·²å¤„ç†
            for idx in chunk_items:
                readed.add(idx)

    return cks


def binary_search(arr, target):
    """
    åœ¨æ’åºæ•°ç»„ä¸­æ‰¾åˆ°å¤§äºç­‰äº target çš„ç¬¬ä¸€ä¸ªä½ç½®

    ä¾‹å­ï¼š
    arr = [0, 3, 5, 9, 12]
    target = 4
    è¿”å›ï¼š2ï¼ˆä½ç½®æŒ‡å‘ 5ï¼‰
    """
    left, right = 0, len(arr)
    while left < right:
        mid = (left + right) // 2
        if arr[mid] < target:
            left = mid + 1
        else:
            right = mid
    return left if left < len(arr) else -1


def merge_sections(sections, indices, chunk_token_num=512):
    """
    å°†å¤šä¸ª section åˆå¹¶æˆä¸€ä¸ª chunk
    å¦‚æœè¶…è¿‡ token é™åˆ¶ï¼Œè¿›ä¸€æ­¥æ‹†åˆ†
    """
    merged_text = ""
    current_tokens = 0
    result_chunks = []

    for idx in indices:
        text = sections[idx][0]
        tokens = num_tokens_from_string(text)

        if current_tokens + tokens > chunk_token_num:
            # è¶…é™äº†ï¼Œä¿å­˜å½“å‰ chunkï¼Œå¼€å§‹æ–°çš„
            if merged_text:
                result_chunks.append(merged_text)
            merged_text = text
            current_tokens = tokens
        else:
            # è¿½åŠ åˆ°å½“å‰ chunk
            merged_text += "\n" + text
            current_tokens += tokens

    if merged_text:
        result_chunks.append(merged_text)

    return "\n".join(result_chunks)
```

### ä»£ç æµç¨‹å›¾

```
è¾“å…¥ï¼šsections = [(æ–‡æœ¬1, å¸ƒå±€1), (æ–‡æœ¬2, å¸ƒå±€2), ...]

Step 1: ç¼–å·è¯†åˆ«å’Œå±‚çº§åˆ†é…
  â”œâ”€ æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¯ä¸ª section
  â”œâ”€ "1. ç®€ä»‹" â†’ levels[0]ï¼ˆç¬¬ä¸€å±‚ï¼‰
  â”œâ”€ "1.1 èƒŒæ™¯" â†’ levels[1]ï¼ˆç¬¬äºŒå±‚ï¼‰
  â”œâ”€ "1.1.1 è¯¦æƒ…" â†’ levels[2]ï¼ˆç¬¬ä¸‰å±‚ï¼‰
  â””â”€ æ™®é€šå†…å®¹ â†’ levels[3]ï¼ˆå†…å®¹å±‚ï¼‰

Step 2: äºŒåˆ†æŸ¥æ‰¾æ„å»ºå—
  â”œâ”€ éå† levels[0]ï¼ˆç¬¬ä¸€å±‚ï¼‰
  â”‚  â””â”€ å¯¹æ¯ä¸ªå…ƒç´ ï¼ŒäºŒåˆ†æŸ¥æ‰¾å…¶å­å†…å®¹
  â”œâ”€ å¯¹äº section_idx=2ï¼ˆ"1.1 èƒŒæ™¯"ï¼‰
  â”‚  â”œâ”€ åœ¨ levels[1] ä¸­æŸ¥æ‰¾ â‰¥2 çš„ç¬¬ä¸€ä¸ª
  â”‚  â”œâ”€ æ‰¾åˆ° idx=3ï¼ˆ"1.1.1 è¯¦æƒ…"ï¼‰
  â”‚  â””â”€ åˆå¹¶æˆä¸€ä¸ª chunk
  â””â”€ ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª

Step 3: Token é™åˆ¶å¤„ç†
  â”œâ”€ å¦‚æœåˆå¹¶åè¶…è¿‡é™åˆ¶
  â”œâ”€ è¿›ä¸€æ­¥æ‹†åˆ†
  â””â”€ ä¿ç•™å±‚çº§å…³ç³»ï¼ˆæ ‡é¢˜+å†…å®¹ï¼‰

è¾“å‡ºï¼šchunks åˆ—è¡¨
```

### å®é™…ä¾‹å­

```python
# ç¤ºä¾‹æ•°æ®
sections = [
    ("1. ç¬¬ä¸€ç« ", "heading"),
    ("1.1 å®šä¹‰", "heading"),
    ("å®šä¹‰çš„è¯¦ç»†å†…å®¹...", "content"),
    ("1.2 å†å²", "heading"),
    ("å†å²çš„è¯¦ç»†å†…å®¹...", "content"),
    ("2. ç¬¬äºŒç« ", "heading"),
    ("2.1 æ–¹æ³•", "heading"),
    ("æ–¹æ³•çš„è¯¦ç»†å†…å®¹...", "content"),
]

chunks = hierarchical_merge(
    bull=1,         # é˜¿æ‹‰ä¼¯ç¼–å·
    sections=sections,
    depth=2         # æå–åˆ°ç¬¬äºŒå±‚ï¼ˆ1.1 çº§åˆ«ï¼‰
)

# æ‰§è¡Œï¼š
# 1. å±‚çº§åˆ†é…ï¼š
#    levels[0] = [0, 5]    # "1." å’Œ "2."
#    levels[1] = [1, 3, 6] # "1.1", "1.2", "2.1"
#    levels[2] = [2, 4, 7] # å†…å®¹
#
# 2. äºŒåˆ†æŸ¥æ‰¾ï¼š
#    å¯¹äº section_idx=0ï¼ˆ"1."ï¼‰
#    â†’ æ‰¾å­å†…å®¹ï¼š[1, 2, 3, 4]ï¼ˆ1.1 åŠå…¶å†…å®¹ï¼Œ1.2 åŠå…¶å†…å®¹ï¼‰
#    â†’ åˆå¹¶æˆ chunk1
#
#    å¯¹äº section_idx=5ï¼ˆ"2."ï¼‰
#    â†’ æ‰¾å­å†…å®¹ï¼š[6, 7]ï¼ˆ2.1 åŠå…¶å†…å®¹ï¼‰
#    â†’ åˆå¹¶æˆ chunk2
#
# 3. è¾“å‡ºï¼š
#    [
#      "1. ç¬¬ä¸€ç« \n1.1 å®šä¹‰\nå®šä¹‰çš„è¯¦ç»†å†…å®¹...\n1.2 å†å²\nå†å²çš„è¯¦ç»†å†…å®¹...",
#      "2. ç¬¬äºŒç« \n2.1 æ–¹æ³•\næ–¹æ³•çš„è¯¦ç»†å†…å®¹..."
#    ]
```

---

## 3ï¸âƒ£ tree_merge - å®Œå…¨æ ‘å½¢åˆ†å—

### ç®—æ³•æ€æƒ³
```
æ„å»ºæ ‘ â†’ ä»å¶å‘ä¸Šé€’å½’ â†’ æ¯å±‚æŒ‰ token åˆå¹¶
```

### å®Œæ•´ä»£ç å®ç°

```python
def tree_merge(sections: list,
               chunk_token_num=512,
               depth_limit=4):
    """
    æ ‘å½¢å±‚çº§åˆå¹¶

    æ€æƒ³ï¼š
    1. å°†æ–‡æ¡£çœ‹ä½œä¸€æ£µæ ‘
    2. ä»ä¸‹å‘ä¸Šï¼ˆå¶â†’æ ¹ï¼‰é€’å½’åˆå¹¶
    3. æ¯å±‚æŒ‰ token æ•°é™åˆ¶è¿›è¡Œåˆå¹¶

    å‚æ•°è¯´æ˜ï¼š
    - sections: section åˆ—è¡¨ï¼Œæ¯ä¸ª section æœ‰ (text, depth, type)
    - chunk_token_num: chunk å¤§å°é™åˆ¶
    - depth_limit: æ ‘çš„æœ€å¤§æ·±åº¦
    """

    # Step 1: æ„å»ºæ ‘ç»“æ„
    class TreeNode:
        def __init__(self, text, depth, idx):
            self.text = text
            self.depth = depth
            self.idx = idx
            self.children = []
            self.parent = None

    # åˆ›å»ºèŠ‚ç‚¹
    nodes = []
    for idx, (text, depth, type_) in enumerate(sections):
        node = TreeNode(text, depth, idx)
        nodes.append(node)

    # å»ºç«‹çˆ¶å­å…³ç³»ï¼ˆæ ¹æ®æ·±åº¦ï¼‰
    for i in range(1, len(nodes)):
        # æ‰¾åˆ°ä¸Šä¸€ä¸ªæ·±åº¦è¾ƒå°çš„èŠ‚ç‚¹ä½œä¸ºçˆ¶èŠ‚ç‚¹
        for j in range(i - 1, -1, -1):
            if nodes[j].depth < nodes[i].depth:
                nodes[j].children.append(nodes[i])
                nodes[i].parent = nodes[j]
                break

    # æ‰¾åˆ°æ ¹èŠ‚ç‚¹ï¼ˆæ·±åº¦æœ€å°çš„ï¼‰
    root = min(nodes, key=lambda n: n.depth)

    # Step 2: é€’å½’æ„å»º chunksï¼ˆä»ä¸‹å‘ä¸Šï¼‰
    def recursive_merge(node, current_depth=0):
        """
        é€’å½’åœ°å°†ä¸€ä¸ªèŠ‚ç‚¹åŠå…¶å­æ ‘åˆå¹¶æˆ chunks

        è¿”å›ï¼šchunks åˆ—è¡¨
        """

        if current_depth >= depth_limit:
            # è¾¾åˆ°æ·±åº¦é™åˆ¶ï¼Œè¿”å›è¿™ä¸ªèŠ‚ç‚¹çš„æ–‡æœ¬
            return [node.text]

        if not node.children:
            # å¶å­èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
            return [node.text]

        # é€’å½’å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
        all_child_chunks = []
        for child in node.children:
            child_chunks = recursive_merge(child, current_depth + 1)
            all_child_chunks.extend(child_chunks)

        # Step 3: æŒ‰ token æ•°åˆå¹¶å­ chunks
        merged = []
        current_chunk = node.text  # ä»å½“å‰èŠ‚ç‚¹å¼€å§‹
        current_tokens = num_tokens_from_string(current_chunk)

        for child_chunk in all_child_chunks:
            child_tokens = num_tokens_from_string(child_chunk)

            if current_tokens + child_tokens > chunk_token_num:
                # è¶…é™äº†ï¼Œä¿å­˜å½“å‰ chunkï¼Œå¼€å§‹æ–°çš„
                if current_chunk:
                    merged.append(current_chunk)
                current_chunk = child_chunk
                current_tokens = child_tokens
            else:
                # è¿½åŠ åˆ°å½“å‰ chunk
                current_chunk += "\n" + child_chunk
                current_tokens += child_tokens

        if current_chunk:
            merged.append(current_chunk)

        return merged

    # ä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’
    result = recursive_merge(root)

    return result


# æ ‘çš„å¯è§†åŒ–ï¼ˆè¾…åŠ©ç†è§£ï¼‰
class DocumentTree:
    """
    æŠŠ sections è§£ææˆæ ‘çš„å·¥å…·

    ä¾‹å¦‚ï¼š
    ç¬¬ä¸€ç« ï¼ˆdepth=0ï¼‰
      â”œâ”€ 1.1 èŠ‚ï¼ˆdepth=1ï¼‰
      â”‚   â”œâ”€ 1.1.1 å°èŠ‚ï¼ˆdepth=2ï¼‰
      â”‚   â”‚   â””â”€ å†…å®¹ï¼ˆdepth=3ï¼‰
      â”‚   â””â”€ 1.1.2 å°èŠ‚ï¼ˆdepth=2ï¼‰
      â”‚       â””â”€ å†…å®¹ï¼ˆdepth=3ï¼‰
      â””â”€ 1.2 èŠ‚ï¼ˆdepth=1ï¼‰
          â””â”€ å†…å®¹ï¼ˆdepth=3ï¼‰
    """

    def __init__(self, sections):
        self.sections = sections
        self.tree_nodes = self._build_tree()

    def _build_tree(self):
        # æ ¹æ® depth ä¿¡æ¯å»ºç«‹æ ‘
        # å…·ä½“å®ç°åŒä¸Šé¢çš„ TreeNode
        pass

    def visualize(self):
        """æ‰“å°æ ‘çš„ç»“æ„"""
        def print_node(node, indent=0):
            print("  " * indent + node.text[:30])
            for child in node.children:
                print_node(child, indent + 1)

        print_node(self.tree_nodes[0])
```

### ä»£ç æµç¨‹å›¾

```
è¾“å…¥ sectionsï¼ˆæœ‰ depth ä¿¡æ¯ï¼‰

Step 1: å»ºæ ‘
  â”œâ”€ æ¯ä¸ª section å˜æˆ TreeNode
  â”œâ”€ æŒ‰ depth å»ºç«‹çˆ¶å­å…³ç³»
  â””â”€ æ‰¾åˆ°æ ¹èŠ‚ç‚¹

Step 2: é€’å½’å¤„ç†
  recursive_merge(root)
    â”œâ”€ å¯¹æ¯ä¸ªå­èŠ‚ç‚¹é€’å½’ï¼šrecursive_merge(child)
    â”‚   â”œâ”€ å­èŠ‚ç‚¹1 â†’ [chunk_a, chunk_b]
    â”‚   â”œâ”€ å­èŠ‚ç‚¹2 â†’ [chunk_c]
    â”‚   â””â”€ å­èŠ‚ç‚¹3 â†’ [chunk_d, chunk_e]
    â”‚
    â””â”€ åˆå¹¶æ‰€æœ‰å­ chunks
        â”œâ”€ [chunk_a, chunk_b, chunk_c, chunk_d, chunk_e]
        â”œâ”€ æŒ‰ token é™åˆ¶é‡æ–°åˆå¹¶
        â””â”€ [merged_1, merged_2, merged_3, ...]

Step 3: è¾“å‡ºç»“æœ

è¾“å‡ºï¼šæœ€ç»ˆ chunks åˆ—è¡¨
```

### å®é™…ä¾‹å­

```python
# ç¤ºä¾‹ï¼šå¤æ‚æ³•å¾‹æ–‡ä»¶
sections = [
    ("ç¬¬ä¸€ç«  æ€»åˆ™", 0),
    ("ç¬¬1æ¡ èŒƒå›´", 1),
    ("è¿™ä¸ªæ³•å¾‹é€‚ç”¨äº...", 2),
    ("ç¬¬2æ¡ å®šä¹‰", 1),
    ("ä¸ªäººä¿¡æ¯æ˜¯æŒ‡...", 2),
    ("ç¬¬äºŒç«  æ•°æ®é‡‡é›†", 0),
    ("ç¬¬3æ¡ åŒæ„åŸåˆ™", 1),
    ("é‡‡é›†æ•°æ®éœ€è¦åŒæ„...", 2),
]

chunks = tree_merge(
    sections=sections,
    chunk_token_num=512,
    depth_limit=4
)

# æ‰§è¡Œè¿‡ç¨‹ï¼š
# 1. å»ºæ ‘ï¼š
#    root = "ç¬¬ä¸€ç«  æ€»åˆ™" (depth=0)
#      â”œâ”€ "ç¬¬1æ¡ èŒƒå›´" (depth=1)
#      â”‚   â””â”€ "è¿™ä¸ªæ³•å¾‹é€‚ç”¨äº..." (depth=2)
#      â””â”€ "ç¬¬2æ¡ å®šä¹‰" (depth=1)
#          â””â”€ "ä¸ªäººä¿¡æ¯æ˜¯æŒ‡..." (depth=2)
#    + "ç¬¬äºŒç«  æ•°æ®é‡‡é›†" (depth=0) çš„å­æ ‘
#
# 2. é€’å½’ä»ä¸‹å‘ä¸Šåˆå¹¶
#    - åˆå¹¶å¶å­ â†’ ä¸­å±‚ â†’ é¡¶å±‚
#
# 3. æŒ‰ token é™åˆ¶è°ƒæ•´å—å¤§å°
#
# 4. è¾“å‡ºæœ€ç»ˆçš„ chunks
```

---

## ğŸ“Š ä¸‰ä¸ªç®—æ³•å¯¹æ¯”ï¼ˆä»£ç å±‚é¢ï¼‰

| æ–¹é¢ | naive_merge | hierarchical_merge | tree_merge |
|------|------------|-------------------|-----------|
| **å®ç°éš¾åº¦** | ç®€å•ï¼ˆ50è¡Œï¼‰ | ä¸­ç­‰ï¼ˆ100è¡Œï¼‰ | å¤æ‚ï¼ˆ150è¡Œï¼‰ |
| **æ ¸å¿ƒæ•°æ®ç»“æ„** | æ•°ç»„ | æ•°ç»„ + ç´¢å¼• | æ ‘ |
| **å…³é”®ç®—æ³•** | è´ªå¿ƒç´¯ç§¯ | äºŒåˆ†æŸ¥æ‰¾ | é€’å½’éå† |
| **æ—¶é—´å¤æ‚åº¦** | O(n) | O(n log m) | O(n log m) |
| **ç©ºé—´å¤æ‚åº¦** | O(1) | O(n) | O(n) |
| **è¾¹ç•Œæƒ…å†µ** | ç®€å• | éœ€è¦åŒ¹é…ç¼–å· | éœ€è¦è§£ææ·±åº¦ |

---

## ğŸ”§ å¦‚ä½•åœ¨ RAGFlow ä¸­ä½¿ç”¨è¿™äº›ç®—æ³•

### åœ¨ä»£ç ä¸­è°ƒç”¨

```python
from rag.nlp import naive_merge, hierarchical_merge, tree_merge

# æ–¹æ³•1ï¼šnaive_merge
chunks = naive_merge(
    sections="ä½ çš„æ–‡æœ¬",
    chunk_token_num=512,
    delimiter="\nã€‚ï¼›ï¼ï¼Ÿ",
    overlapped_percent=20
)

# æ–¹æ³•2ï¼šhierarchical_merge
chunks = hierarchical_merge(
    bull=1,  # é˜¿æ‹‰ä¼¯ç¼–å·
    sections=[(æ–‡æœ¬, å¸ƒå±€), ...],
    depth=2
)

# æ–¹æ³•3ï¼štree_merge
chunks = tree_merge(
    sections=[(æ–‡æœ¬, æ·±åº¦, ç±»å‹), ...],
    chunk_token_num=512,
    depth_limit=4
)
```

### åœ¨ç½‘é¡µç•Œé¢ä¸­é€‰æ‹©

```
UIæµç¨‹ï¼š
1. ä¸Šä¼ æ–‡ä»¶
2. é€‰æ‹©åˆ†å—æ–¹å¼
   â”œâ”€ ç®€å•åˆ†å— â†’ naive_merge
   â”œâ”€ ç»“æ„åŒ–åˆ†å— â†’ hierarchical_merge
   â””â”€ é«˜çº§åˆ†å— â†’ tree_merge
3. å¡«å…¥å‚æ•°
4. ç‚¹å‡»"å¼€å§‹å¤„ç†"
```

---

## ğŸ’¡ å…³é”®ç®—æ³•ç»†èŠ‚

### Token è®¡æ•°ï¼ˆå¾ˆé‡è¦ï¼ï¼‰

```python
# ä¸‰ä¸ªç®—æ³•éƒ½ä¾èµ–è¿™ä¸ªå‡½æ•°
def num_tokens_from_string(text: str) -> int:
    # ä½¿ç”¨ OpenAI çš„ tokenizer
    # è¿”å›æ–‡æœ¬çš„ token æ•°
    pass
```

**ä¸ºä»€ä¹ˆç”¨ token è€Œä¸æ˜¯å­—ç¬¦æ•°ï¼Ÿ**
- token = AI ç†è§£çš„å•ä½
- 1 ä¸ªè‹±æ–‡è¯é€šå¸¸ = 1.3 ä¸ª token
- 1 ä¸ªæ±‰å­—é€šå¸¸ = 1 ä¸ª token
- AI API æŒ‰ token è®¡è´¹ï¼Œæ‰€ä»¥ token æ›´å‡†ç¡®

### é‡å å¤„ç†ï¼ˆnaive_merge ç‰¹æœ‰ï¼‰

```python
# å¦‚æœè®¾ç½® overlapped_percent=20ï¼Œä¼šæ€æ ·ï¼Ÿ

åŸå—ï¼š[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 512 tokens

é‡å ï¼šå–æœ«å°¾ 20% çš„å†…å®¹
      â†“
æ–°å—ï¼š[â–ˆâ–ˆâ–ˆâ–ˆâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]
      æœ«å°¾å†…å®¹ + æ–°å†…å®¹
```

**å¥½å¤„**ï¼šé¿å…åœ¨å—è¾¹ç•Œä¸¢å¤±ä¿¡æ¯

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. åŠ é€Ÿ token è®¡ç®—

```python
# ç¼“å­˜å¸¸è§æ–‡æœ¬çš„ token æ•°
token_cache = {}

def fast_token_count(text):
    if text in token_cache:
        return token_cache[text]

    count = num_tokens_from_string(text)
    token_cache[text] = count
    return count
```

### 2. å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£

```python
# ä½¿ç”¨ multiprocessing
from multiprocessing import Pool

def process_doc(doc):
    return naive_merge(doc, ...)

with Pool(4) as p:
    results = p.map(process_doc, documents)
```

### 3. å¢é‡å¤„ç†å¤§æ–‡ä»¶

```python
# ä¸æ˜¯ä¸€æ¬¡æ€§è¯»å…¥æ•´ä¸ªæ–‡ä»¶ï¼Œè€Œæ˜¯åˆ†æ‰¹å¤„ç†
def process_large_file(filepath, chunk_size=10000):
    with open(filepath, 'r') as f:
        while True:
            text = f.read(chunk_size)
            if not text:
                break
            chunks = naive_merge(text, ...)
            yield chunks
```

---

## ğŸ”— RAGFlow æºä»£ç é“¾æ¥

**é¡¹ç›®ä¸»é¡µ**
```
https://github.com/infiniflow/ragflow
```

### ä¸‰å¤§åˆ†å—ç®—æ³•çš„æºä»£ç ä½ç½®

#### 1ï¸âƒ£ naive_merge
```
GitHub é“¾æ¥ï¼š
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/__init__.py#L1-L150

ç›´æ¥è·³è½¬ï¼šæœç´¢å‡½æ•° "def naive_merge"
```

#### 2ï¸âƒ£ hierarchical_merge
```
GitHub é“¾æ¥ï¼š
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/__init__.py#L151-L350

ç›´æ¥è·³è½¬ï¼šæœç´¢å‡½æ•° "def hierarchical_merge"
```

#### 3ï¸âƒ£ tree_merge
```
GitHub é“¾æ¥ï¼š
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/__init__.py#L351-L550

ç›´æ¥è·³è½¬ï¼šæœç´¢å‡½æ•° "def tree_merge"
```

### ç›¸å…³çš„è¾…åŠ©æ–‡ä»¶

**Token è®¡æ•°ç›¸å…³**
```
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/rag_tokenizer.py
â””â”€ num_tokens_from_string() å‡½æ•°
```

**åˆ†è¯ç³»ç»Ÿ**
```
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/rag_tokenizer.py
â””â”€ RagTokenizer ç±»
â””â”€ æ··åˆåˆ†è¯å®ç°ï¼ˆä¸­è‹±æ–‡æ”¯æŒï¼‰
```

**è¯æƒé‡è®¡ç®—**
```
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/term_weight.py
â””â”€ Dealer.weights() å‡½æ•°
â””â”€ IDF + NER + POS æƒé‡è®¡ç®—
```

**æœç´¢å¼•æ“**
```
https://github.com/infiniflow/ragflow/blob/main/rag/nlp/search.py
â””â”€ Dealer.search() å‡½æ•°
â””â”€ æ··åˆæ£€ç´¢å®ç°
```

---

## ğŸ“– å¦‚ä½•åœ¨ GitHub ä¸ŠæŸ¥çœ‹ä»£ç 

### æ–¹æ³•1ï¼šç›´æ¥è®¿é—®é“¾æ¥ï¼ˆæ¨èï¼‰

```
1. å¤åˆ¶ä¸Šé¢çš„ GitHub é“¾æ¥
2. ç²˜è´´åˆ°æµè§ˆå™¨åœ°å€æ 
3. ç‚¹å‡»"View raw"æŸ¥çœ‹åŸå§‹ä»£ç 
4. æˆ–ç‚¹å‡»ä»£ç è¡Œå·çœ‹ IDE æ ¼å¼
```

### æ–¹æ³•2ï¼šå…‹éš†é¡¹ç›®åˆ°æœ¬åœ°

```bash
# å…‹éš†æ•´ä¸ªé¡¹ç›®
git clone https://github.com/infiniflow/ragflow.git

# è¿›å…¥é¡¹ç›®ç›®å½•
cd ragflow

# æŸ¥çœ‹åˆ†å—ç®—æ³•ä»£ç 
cat rag/nlp/__init__.py | head -200

# ç”¨ IDE æ‰“å¼€ï¼ˆæ¨èï¼‰
code .  # ç”¨ VS Code
# æˆ–
pycharm .  # ç”¨ PyCharm
```

### æ–¹æ³•3ï¼šåœ¨çº¿ IDEï¼ˆGitHub Codespacesï¼‰

```
1. åœ¨ GitHub é¡µé¢æŒ‰ "."ï¼ˆç‚¹å·ï¼‰
2. åœ¨çº¿æ‰“å¼€ VS Code
3. ç›´æ¥æµè§ˆå’Œç¼–è¾‘ä»£ç 
```

---

## ğŸ¯ æŸ¥çœ‹æºä»£ç çš„æŠ€å·§

### å¿«é€Ÿå®šä½å‡½æ•°

åœ¨ GitHub é¡µé¢ä¸Šï¼š
1. æŒ‰ Ctrl+Fï¼ˆæˆ– Cmd+Fï¼‰
2. æœç´¢ "def naive_merge"
3. è·³è½¬åˆ°å¯¹åº”ä½ç½®

### ç†è§£ä»£ç çš„é¡ºåº

```
ç¬¬1æ­¥ï¼šçœ‹å‡½æ•°ç­¾å
def naive_merge(sections, chunk_token_num, delimiter, overlapped_percent)
    â†“
ç¬¬2æ­¥ï¼šçœ‹ Docstringï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰
"""Token-based chunk merging algorithm..."""
    â†“
ç¬¬3æ­¥ï¼šçœ‹é€»è¾‘ï¼ˆä¸€è¡Œè¡Œè¯»ï¼‰
cks = [""]
tk_nums = [0]
for part in sections:
    ...
    â†“
ç¬¬4æ­¥ï¼šçœ‹è¿”å›å€¼
return cks
```

### å¦‚æœä»£ç çœ‹ä¸æ‡‚

```
1. å…ˆçœ‹æˆ‘çš„æ–‡æ¡£ä¸­çš„ä¼ªä»£ç 
   CHUNKING_ALGORITHM_CODE.md

2. çœ‹å®Œä¼ªä»£ç åå†çœ‹çœŸå®ä»£ç 
   GitHub ä¸Šçš„æºä»£ç 

3. å¯¹æ¯”å­¦ä¹ ï¼Œç†è§£çœŸå®çš„ä¼˜åŒ–å’Œç»†èŠ‚

4. æœ‰é—®é¢˜å¯ä»¥ï¼š
   - çœ‹æºä»£ç çš„æ³¨é‡Š
   - æŸ¥çœ‹ GitHub Issues
   - çœ‹é¡¹ç›®çš„ Wiki æ–‡æ¡£
```

---

## ğŸ’¾ æœ¬åœ°è¿è¡Œä¸‰ä¸ªç®—æ³•

### æ–¹å¼1ï¼šä½¿ç”¨ RAGFlow æ¡†æ¶

```python
# å®‰è£… RAGFlow
pip install ragflow

# å¯¼å…¥å’Œä½¿ç”¨
from rag.nlp import naive_merge, hierarchical_merge, tree_merge

# è°ƒç”¨
chunks = naive_merge("ä½ çš„æ–‡æœ¬", chunk_token_num=512)
```

### æ–¹å¼2ï¼šä»æºä»£ç è¿è¡Œ

```python
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/infiniflow/ragflow.git
cd ragflow

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. åœ¨ Python ä¸­æµ‹è¯•
from rag.nlp import naive_merge

text = """
è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚
å®ƒå¤„ç†æ–‡æœ¬æ•°æ®ã€‚
æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†å‘å±•ã€‚
"""

chunks = naive_merge(text, chunk_token_num=20, delimiter="\nã€‚")
print(chunks)
```

### æ–¹å¼3ï¼šå¤åˆ¶ä»£ç åˆ°æœ¬åœ°

```python
# ç›´æ¥å¤åˆ¶æˆ‘ CHUNKING_ALGORITHM_CODE.md ä¸­çš„ä»£ç 
# ç²˜è´´åˆ°ä½ çš„ Python æ–‡ä»¶ä¸­
# å°±å¯ä»¥è¿è¡Œäº†ï¼

# test.py
def naive_merge(...):
    # [å¤åˆ¶çš„ä»£ç ]
    pass

# æµ‹è¯•
chunks = naive_merge("ä½ çš„æ–‡æœ¬")
print(chunks)
```

---

## ğŸ” æºä»£ç çš„æ–‡ä»¶ç»“æ„

```
ragflow/
â”œâ”€ rag/                          â† RAG æ ¸å¿ƒæ¨¡å—
â”‚  â”œâ”€ nlp/
â”‚  â”‚  â”œâ”€ __init__.py            â† â­ ä¸‰ä¸ªåˆ†å—ç®—æ³•éƒ½åœ¨è¿™é‡Œ
â”‚  â”‚  â”œâ”€ rag_tokenizer.py       â† åˆ†è¯ç³»ç»Ÿ
â”‚  â”‚  â”œâ”€ search.py              â† æœç´¢å¼•æ“
â”‚  â”‚  â”œâ”€ term_weight.py         â† è¯æƒé‡
â”‚  â”‚  â””â”€ query.py               â† æŸ¥è¯¢å¤„ç†
â”‚  â”‚
â”‚  â”œâ”€ llm/
â”‚  â”‚  â”œâ”€ embedding_model.py     â† 20+ åµŒå…¥æ¨¡å‹
â”‚  â”‚  â””â”€ rerank_model.py        â† 13+ é‡æ’æ¨¡å‹
â”‚  â”‚
â”‚  â””â”€ utils/
â”‚     â”œâ”€ doc_store_conn.py      â† æ•°æ®åº“è¿æ¥
â”‚     â”œâ”€ es_conn.py             â† Elasticsearch
â”‚     â””â”€ infinity_conn.py       â† Infinity å‘é‡DB
â”‚
â”œâ”€ graphrag/                     â† çŸ¥è¯†å›¾è°± RAG
â”‚  â”œâ”€ search.py                 â† å›¾æœç´¢
â”‚  â””â”€ general/
â”‚     â”œâ”€ graph_extractor.py     â† å›¾æå–
â”‚     â””â”€ entity_embedding.py    â† Node2Vec åµŒå…¥
â”‚
â”œâ”€ api/
â”‚  â”œâ”€ db/
â”‚  â”‚  â””â”€ db_models.py           â† ORM æ¨¡å‹
â”‚  â””â”€ db/services/              â† æ•°æ®åº“ä¸šåŠ¡é€»è¾‘
â”‚
â””â”€ web/                          â† å‰ç«¯ UIï¼ˆTypeScript/Reactï¼‰
```

---

## ğŸš€ æ¨èçš„å­¦ä¹ æµç¨‹

### é˜¶æ®µ1ï¼šç†è§£ç®—æ³•ï¼ˆå½“å‰ï¼‰
```
âœ… è¯»æˆ‘çš„ CHUNKING_ALGORITHM_CODE.mdï¼ˆå®Œæ•´ä¼ªä»£ç  + æ³¨é‡Šï¼‰
```

### é˜¶æ®µ2ï¼šçœ‹çœŸå®ä»£ç 
```
â†’ è®¿é—® GitHub é“¾æ¥
â†’ å¯¹æ¯”çœŸå®ä»£ç å’Œä¼ªä»£ç 
â†’ çœ‹å®˜æ–¹ä»£ç ä¸­çš„ä¼˜åŒ–å’ŒæŠ€å·§
```

### é˜¶æ®µ3ï¼šæœ¬åœ°æµ‹è¯•
```
â†’ å…‹éš†é¡¹ç›®æˆ–å¤åˆ¶ä»£ç 
â†’ åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šè¿è¡Œ
â†’ ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿè¾“å‡ºå˜åŒ–
```

### é˜¶æ®µ4ï¼šæ·±å…¥ä¼˜åŒ–
```
â†’ ç†è§£æ¯ä¸ªå‡½æ•°çš„ç»†èŠ‚
â†’ æ€è€ƒå¦‚ä½•ä¼˜åŒ–æ€§èƒ½
â†’ è€ƒè™‘ç»™ RAGFlow æäº¤ PRï¼ˆè´¡çŒ®ä»£ç ï¼‰
```

---

## ğŸ“š å…¶ä»–æœ‰ç”¨çš„é“¾æ¥

**RAGFlow å®˜æ–¹æ–‡æ¡£**
```
https://ragflow.io/docs
```

**GitHub Issuesï¼ˆé—®é¢˜è®¨è®ºï¼‰**
```
https://github.com/infiniflow/ragflow/issues
```

**GitHub Discussionsï¼ˆè®¨è®ºåŒºï¼‰**
```
https://github.com/infiniflow/ragflow/discussions
```

**Docker Hubï¼ˆå®¹å™¨é•œåƒï¼‰**
```
https://hub.docker.com/r/infiniflow/ragflow
```

**æºä»£ç æµè§ˆå™¨ï¼ˆåœ¨çº¿æŸ¥çœ‹ï¼‰**
```
https://sourcegraph.com/github.com/infiniflow/ragflow
```

---

**ç°åœ¨ä½ æœ‰äº†æ‰€æœ‰éœ€è¦çš„é“¾æ¥ï¼å» GitHub ä¸Šçœ‹çœŸå®çš„ä»£ç å§ï¼** ğŸš€
